# Initialize Docker Swarm on manager
- name: Initialize Swarm (manager)
  docker_swarm:
    state: present
    advertise_addr: "{{ ansible_host }}"
  when: hostvars[inventory_hostname].role == "dashboard"
  register: swarm_init

# Save Swarm join tokens globally (run once)
- name: Save Swarm join tokens
  set_fact:
    manager_join_token: "{{ swarm_init.swarm_facts.JoinTokens.Manager }}"
    worker_join_token: "{{ swarm_init.swarm_facts.JoinTokens.Worker }}"
  when: hostvars[inventory_hostname].role == "dashboard"
  run_once: true

# Ensure workers leave any existing swarm
- name: Ensure worker nodes leave any existing swarm
  docker_swarm:
    state: absent
    force: true
  when: hostvars[inventory_hostname].role == "worker"

# Remove old/duplicate nodes on manager (except current Ready workers)
- name: Remove duplicate swarm nodes aggressively
  shell: |
    docker node ls --format '{{"{{.ID}} {{.Hostname}} {{.Status}}"}}' | sort -k2,2 -u -s |
    awk '
      {
        if (!seen[$2]) {
          seen[$2]=$1
        } else {
          print $1
        }
      }' |
    while read NODE_ID; do
      echo "Removing duplicate node $NODE_ID" >&2
      docker node rm -f "$NODE_ID" || true
    done
    dups=$(docker node ls --format '{{"{{.Hostname}}"}}' | sort | uniq -d)
    if [ -n "$dups" ]; then
      echo "Still have duplicate nodes: $dups" >&2
      exit 1
    fi
  delegate_to: node1
  run_once: true

# Verify only one node per hostname remains (before labeling)
- name: Verify only one node per hostname remains
  shell: |
    count=$(docker node ls --format '{{'{{.Hostname}}'}}' | sort | uniq -d)
    if [ -n "$count" ]; then
      echo "Duplicate nodes still exist: $count" >&2
      exit 1
    fi
  delegate_to: node1
  run_once: true

# Join worker nodes to swarm
- name: Join worker nodes to swarm
  docker_swarm:
    state: join
    join_token: "{{ hostvars['node1'].worker_join_token }}"
    remote_addrs: ["{{ hostvars['node1'].ansible_host }}"]
  when: hostvars[inventory_hostname].role == "worker"

# Wait for workers to appear in Swarm
- name: Wait for workers to register in Swarm
  shell: |
    for i in $(seq 1 30); do
      docker node ls --format '{{"{{.Hostname}}"}}' | grep -q "{{ ansible_hostname }}" && exit 0
      sleep 5
    done
    exit 1
  delegate_to: node1
  when: hostvars[inventory_hostname].role == "worker"

# Label nodes
- name: Label nodes according to their role
  docker_node:
    hostname: "{{ ansible_hostname }}"
    labels:
      role: "{{ hostvars[inventory_hostname].role }}"
  delegate_to: node1
  run_once: false

# Fix Traefik scheduling to use node label
- name: Fix Traefik constraint to use node label
  shell: |
    {% raw %}
    if docker service ls --format '{{.Name}}' | grep -q '^traefik$'; then
      docker service update \
        --constraint-rm 'node.role == dashboard' \
        --constraint-add 'node.labels.role == dashboard' \
        traefik || true
    else
      echo "Traefik service not found, skipping update"
    fi
    {% endraw %}
  delegate_to: node1
  run_once: true

# Force Traefik to restart and clear old tasks
- name: Force Traefik to restart and clear old tasks
  shell: |
    {% raw %}
    if docker service ls --format '{{.Name}}' | grep -q '^traefik$'; then
      docker service update --force traefik
    else
      echo "Traefik service not found, skipping update"
    fi
    {% endraw %}
  delegate_to: node1
  run_once: true

# Ensure Traefik is running with 1 replica
- name: Ensure Traefik is running with 1 replica
  shell: |
    {% raw %}
    if docker service ls --format '{{.Name}}' | grep -q '^traefik$'; then
      docker service scale traefik=1
    else
      echo "Traefik service not found, skipping scale"
    fi
    {% endraw %}
  delegate_to: node1
  run_once: true
